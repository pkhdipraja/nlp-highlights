\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{setspace}
\setlength{\parskip}{10pt}
\title{Sequence to Sequence Modelling}
\author{Patrick Kahardipraja}
\date{September 2019}
\onehalfspacing

\begin{document}
% Make essay with style of literature review and blogposts

\maketitle
% Notes to remember: explain RNN and LSTM a bit?
% Problem of statistical machine translation?
% Applying seq2seq to many languages?
% Find seq2seq literature review
%The idea of machine translation is central to.

Mapping of a sequence to another is an important paradigm because of the vast amount of problems that can be formulated in this manner. For instance, in automatic speech recognition chunks of speech signals can be mapped to sequence of phonemes while in machine translation, a sequence of words in one language can be mapped to another language. This pattern also extend to many other tasks such as text summarization, question answering, and image caption generation. In this paper, I will attempt to distill how sequence to sequence learning works and the motivation behind it, with a particular focus on machine translation.


\section*{Introduction}
Brief about SMT..(Prior to seq2seq)NN dead in 1990s. Resurgence of Deep Neural Networks (DNNs) in the last few years have been attributed to gpu power and availability of good dataset (ImageNet). Equipped with the capability to learn features automatically with multiple levels of representation, DNNs are already proven to achieve excellent performance on task such as object recognition (AlexNet). However, although powerful.. 

\section*{Recurrent Neural Networks}

\section*{Seq2Seq Model}

\section*{What's Next?}

\section*{Conclusion}
\end{document}