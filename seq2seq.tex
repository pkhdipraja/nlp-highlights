\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{setspace}
\setlength{\parskip}{10pt}
\title{Sequence to Sequence Modelling}
\author{Patrick Kahardipraja}
\date{September 2019}
\onehalfspacing

\begin{document}
% Make essay with style of literature review and blogposts

\maketitle
% Notes to remember: explain RNN and LSTM a bit?
% Problem of statistical machine translation?
% Applying seq2seq to many languages?
% Find seq2seq literature review
% Explain distributional representation?
% Tell about Google NMT?
% encoder - decoder
%The idea of machine translation is central to.

Mapping of a sequence to another sequence is an important paradigm because of the vast amount of problems that can be formulated in this manner. For instance, in automatic speech recognition (ASR), chunks of speech signals can be mapped to sequence of phonemes while in machine translation, a sequence of words in one language can be mapped to another language. Interestingly, many other tasks such as text summarization, question answering, and image caption generation can be phrased as a sequence to sequence problem. In this paper, I will attempt to distill how sequence to sequence learning works and the motivation behind it, with a particular focus on machine translation.


\section*{Introduction}

Brief about SMT..(Prior to NMT)NN dead in 1990s. Resurgence of Deep Neural Networks (DNNs) in the last few years have been attributed to gpu power and availability of good dataset (ImageNet). Equipped with the capability to learn features automatically with multiple levels of representation, DNNs are already proven to achieve excellent performance on task such as object recognition (AlexNet). However, although powerful.. 

Prior to neural machine translation (NMT), phrase-based stastistical machine translation (SMT) systems are widely used as it offers reliable performance. Despite its success, most of them are extremely complex and require a huge amount of effort, as it is often tailored to a specific language pair and do not generalize well to another languages. Furthermore, a lot of feature engineering are required in order to capture a specific language phenomena, which prompt researchers to explore another approach.

The resurgence of deep neural networks (DNNs) in early 2010s, thanks to faster, parallel computation using GPUs and availability of large and high-quality datasets, bring a new wave of enthusiasm in deep models. With the capability to learn features automatically with multiple, hierarchical representation, DNNs achieve excellent performance on difficult tasks in computer vision [AlexNet] and speech recognition []. Albeit powerful, DNNs has its own limitation, as



 

\section*{Recurrent Neural Networks}

\section*{Seq2Seq Model}
The first work [Cho et al.]

\section*{Recent Advances} % also include problems with Seq2Seq?

\section*{Conclusion}
\end{document}